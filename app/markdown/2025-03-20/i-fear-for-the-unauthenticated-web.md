# I fear for the unauthenticated web

LLM and AI companies seem to all be in a race to
[breathe the last breath of air](https://drewdevault.com/2025/03/17/2025-03-17-Stop-externalizing-your-costs-on-me.html) in every room
they stumble into. This practice started with larger
websites, ones that already had protection from
malicious usage like denial-of-service and abuse in the form
of services like Cloudflare or Fastly.

But the list of targets has been getting longer.
At this point we're seeing LLM and AI scrapers
[targeting small project forges](https://thelibre.news/foss-infrastructure-is-under-attack-by-ai-companies/) like the [GNOME GitLab server](https://gitlab.gnome.org/GNOME).

How long until scrapers start hammering Mastodon servers? Individual websites?
Are we going to have to require authentication or JavaScript challenges on
every web page from here on out?

All this for what, shitty chat bots? What an awful thing that these companies are
doing to the web.

I suggest everyone that uses cloud infrastructure for hosting set-up
a billing limit to avoid an unexpected bill in case they're
caught in the cross-hairs of a negligent company.
All the abusers anonymize their usage at this point, so good luck trying to
get compensated for damages.
